{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run--> run\n",
      "runner--> runner\n",
      "running--> run\n",
      "ran--> ran\n",
      "runs--> run\n",
      "easily--> easili\n",
      "fairly--> fairli\n"
     ]
    }
   ],
   "source": [
    "# And here we give it different and similar words, and we use the stem to analyze them.\n",
    "\n",
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+'--> '+ p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run-->run\n",
      "runner-->runner\n",
      "running-->run\n",
      "ran-->ran\n",
      "runs-->run\n",
      "easily-->easili\n",
      "fairly-->fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "words = ['run', 'runner', 'running', 'ran', 'runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+'-->'+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous --> generous\n",
      "generation --> generat\n",
      "generously --> generous\n",
      "generate --> generat\n"
     ]
    }
   ],
   "source": [
    "words = ['generous', 'generation', 'generously', 'generate']\n",
    "\n",
    "for word in words:\n",
    "    print(word+ ' --> ' + s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "wa\n",
      "be\n",
      "been\n",
      "are\n",
      "were\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "words = [\"is\", \"was\", \"be\", \"been\", \"are\", \"were\"]\n",
    "\n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "was\n",
      "be\n",
      "been\n",
      "ar\n",
      "wer\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(ls.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "book\n",
      "book\n",
      "book\n",
      "booker\n",
      "bookstor\n"
     ]
    }
   ],
   "source": [
    "words = ['book', 'booking', 'booked', 'books', 'booker', 'bookstore']\n",
    "\n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "book\n",
      "book\n",
      "book\n",
      "book\n",
      "bookst\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(ls.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had\n",
      "you\n",
      "book\n",
      "the\n",
      "air\n",
      "book\n",
      "yet\n",
      "?\n",
      "if\n",
      "not\n",
      "try\n",
      "to\n",
      "book\n",
      "it\n",
      "asap\n",
      "sint\n",
      "book\n",
      "wil\n",
      "be\n",
      "out\n",
      "of\n",
      "book\n"
     ]
    }
   ],
   "source": [
    "sentence = 'had you booked the air booking yet ? if not try to book it ASAP since booking will be out of books'\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "for word in words:\n",
    "    print(ls.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 PorterStemmer        LancasterStemmer    \n",
      "friend               friend               friend              \n",
      "friendship           friendship           friend              \n",
      "friends              friend               friend              \n",
      "friendships          friendship           friend              \n",
      "stabil               stabil               stabl               \n",
      "destabilize          destabil             dest                \n",
      "misunderstanding     misunderstand        misunderstand       \n",
      "railroad             railroad             railroad            \n",
      "moonlight            moonlight            moonlight           \n",
      "football             footbal              footbal             \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \n",
    "\"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20}\".format(\"Word\", \"PorterStemmer\", \"LancasterStemmer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20}\".format(word, ps.stem(word), ls.stem(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t 95 \t 4690420944186131903 \t I\n",
      "am \t 87 \t 10382539506755952630 \t be\n",
      "a \t 90 \t 11901859001352538922 \t a\n",
      "runner \t 92 \t 12640964157389618806 \t runner\n",
      "running \t 100 \t 12767647472892411841 \t run\n",
      "in \t 85 \t 3002984154512732771 \t in\n",
      "a \t 90 \t 11901859001352538922 \t a\n",
      "race \t 92 \t 8048469955494714898 \t race\n",
      "because \t 98 \t 16950148841647037698 \t because\n",
      "I \t 95 \t 4690420944186131903 \t I\n",
      "love \t 100 \t 3702023516439754181 \t love\n",
      "to \t 94 \t 3791531372978436496 \t to\n",
      "run \t 100 \t 12767647472892411841 \t run\n",
      "since \t 98 \t 10066841407251338481 \t since\n",
      "I \t 95 \t 4690420944186131903 \t I\n",
      "ran \t 100 \t 12767647472892411841 \t run\n",
      "today \t 92 \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc1 = nlp(\"I am a runner running in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t',token.pos, '\\t',token.lemma, '\\t',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f\"{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "saw          VERB   11925638236994514241   see\n",
      "eighteen     NUM    9609336664675087640    eighteen\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"I saw eighteen mice today!\")\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "catchi\n",
      "radius\n",
      "foot\n",
      "speech\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"cats\",\"catchi\",\"radii\",\"feet\",\"speech\",\"runner\"]\n",
    "\n",
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meeting\n",
      "meet\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"meeting\", \"n\"))\n",
    "print(lemmatizer.lemmatize(\"meeting\",\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_net = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'was',\n",
       " 'running',\n",
       " 'and',\n",
       " 'eating',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " 'He',\n",
       " 'has',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swimming',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_word = word_tokenize(sentence)\n",
    "for word in sentence_word:\n",
    "    if word in punctuations:\n",
    "        sentence_word.remove(word)\n",
    "\n",
    "sentence_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\")) \n",
    "for word in sentence_word: \n",
    " print (\"{0:20}{1:20}\".format(word,word_net.lemmatize(word))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_word: \n",
    " print (\"{0:20}{1:20}\".format(word,word_net.lemmatize(word, pos=\"v\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahmed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
