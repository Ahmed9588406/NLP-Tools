{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665a81c-9f6f-47fc-989c-b315a4218c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf1824-e999-41b5-b8b5-6f7b60cd5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in your terminal python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "825d344f-c2e5-4e1e-88e3-cdbf16d625d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d1a154-f7d9-4725-b242-5f643e0fc1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n",
      "Xxxxx\n",
      "True\n",
      "False\n",
      "---------\n",
      "is\n",
      "xx\n",
      "True\n",
      "True\n",
      "---------\n",
      "looking\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "---------\n",
      "at\n",
      "xx\n",
      "True\n",
      "True\n",
      "---------\n",
      "buying\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "---------\n",
      "U.S.\n",
      "X.X.\n",
      "False\n",
      "False\n",
      "---------\n",
      "startup\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "---------\n",
      "for\n",
      "xxx\n",
      "True\n",
      "True\n",
      "---------\n",
      "$\n",
      "$\n",
      "False\n",
      "False\n",
      "---------\n",
      "6\n",
      "d\n",
      "False\n",
      "False\n",
      "---------\n",
      "million\n",
      "xxxx\n",
      "True\n",
      "False\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Here we give it sentence to process\n",
    "doc = nlp(\"Tesla is looking at buying U.S. startup for $6 million\")\n",
    "\n",
    "\"\"\" \n",
    "Here he separates each word from another, and it is a smart separation. Even though the $6 is attached, he understands that each word is separate.\n",
    " \n",
    "It also determines the form of the word, whether the word is letters or numbers, and whether it is stopwords or not.\n",
    "\"\"\"\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    # print(token.shape) this will return the id and the value if we add _ we will get the vaue\n",
    "    print(token.shape_)\n",
    "    print(token.is_alpha)\n",
    "    print(token.is_stop)\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1dd008-0372-4d00-9a05-41f09c74162e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tesla, is, looking, at, buying, U.S., startup, for)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0], doc[1], doc[2], doc[3], doc[4], doc[5], doc[6], doc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13a35fa-2b11-4bf3-9e6b-984f23def69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is what happens to us while we are making other plans\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp('''Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \n",
    "10 \n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \n",
    "cartoonist Allen Saunders and \n",
    "published in Reader\\'s Digest in 1957, when Lennon was 17. \n",
    "''')\n",
    "\n",
    "life_quote = doc2[20:32]\n",
    "print(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ad3cce-a5e6-48b9-8a48-60b6a8902d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Although,\n",
       " commmonly,\n",
       " attributed,\n",
       " to,\n",
       " John,\n",
       " Lennon,\n",
       " from,\n",
       " his,\n",
       " song,\n",
       " \",\n",
       " Beautiful,\n",
       " Boy,\n",
       " \",\n",
       " ,,\n",
       " ,\n",
       " 10,\n",
       " )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0], doc2[1], doc2[2], doc2[3], doc2[4], doc2[5], doc2[6], doc2[7], doc2[8], doc2[9], doc2[10], doc2[11], doc2[12], doc2[13], doc2[14], doc2[15], doc2[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4adf018d-a850-471c-93de-4e3871a323f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Although"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddae62a7-3a2c-4857-9b98-1535e5d07df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We're moving to L.A.!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring = \"We\\'re moving to L.A.!\"\n",
    "mystring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb0dea7-e12c-48e5-9585-579c1410ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We|'re|moving|to|L.A.|!|"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(mystring)\n",
    "for token in doc3:\n",
    "    print(token.text, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67cfcbda-fa94-4771-adb0-17a78f6e53ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"We're\", 'moving', 'to', 'L.A.!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69863783-48a4-4850-a74a-834761587a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "Send\n",
      "snail\n",
      "-\n",
      "mail\n",
      ",\n",
      "email\n",
      "support@oursite.com\n",
      "or\n",
      "visit\n",
      "us\n",
      "at\n",
      "http://www.oursite.com\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "mystring = \"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\"\n",
    "\n",
    "doc4 = nlp(mystring)\n",
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "355b8871-49cf-4f5a-93ab-f31e5ddc40cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.doc.Doc' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m doc7 \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy dinner was horrible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m doc8 \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour dinner was delicious.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdoc7\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m doc8[\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# Tokens are only read and cannot be write \n",
    "doc7 = nlp(\"My dinner was horrible.\")\n",
    "doc8 = nlp(\"Your dinner was delicious.\")\n",
    "doc7[3] = doc8[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04d5dd7-d639-41f0-a344-3687120e51cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8fea5ca-0bdb-4fd4-bab9-2dff5c810a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From nltk library\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"\"\" \n",
    "Hello Mr. Smith, how are you doing today? The weather is great, \n",
    "and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard. \n",
    "\"\"\" \n",
    "word_tokenize(EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68396048-8bda-42f7-b244-b8e93ba99c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "----------------------------------\n",
      "[]\n",
      "==================================\n",
      "['Hello', 'Mr.', 'Smith,', 'how', 'are', 'you', 'doing', 'today?', 'The', 'weather']\n",
      "----------------------------------\n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n",
      "==================================\n",
      "['and', 'Python', 'is', 'awesome.', 'The', 'sky', 'is', 'pinkish-blue.', 'You', \"shouldn't\"]\n",
      "----------------------------------\n",
      "['and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.']\n",
      "==================================\n",
      "[]\n",
      "----------------------------------\n",
      "[]\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "for line in EXAMPLE_TEXT.split(\"\\n\")[:20]:\n",
    "    print(line.split()[:10])\n",
    "    print(\"----------------------------------\")\n",
    "    print(word_tokenize(line)[:10])\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993aaae-da8e-42ad-bace-e34b5355e0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
